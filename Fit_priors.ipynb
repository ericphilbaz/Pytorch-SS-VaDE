{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "from preprocess import get_mnist, get_webcam\n",
    "from train import TrainerVaDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 128\n",
    "    dataset = 'mnist'\n",
    "    pretrained_path = 'weights/pretrained_parameter.pth'\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "args = Args() # Parsing all the arguments for the training\n",
    "if args.dataset == 'mnist':\n",
    "    dataloader_train, dataloader_test = get_mnist(args)\n",
    "    n_classes = 10\n",
    "else:\n",
    "    dataloader_train, dataloader_test = get_webcam(args)\n",
    "    n_classes = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == 'webcam':\n",
    "    from models_office import Autoencoder, feature_extractor, VaDE\n",
    "    VaDE = VaDE().to(device)\n",
    "    autoencoder = Autoencoder().to(device)\n",
    "    autoencoder.load_state_dict(torch.load('weights/autoencoder_parameters_webcam.pth.tar',\n",
    "                                    map_location=device)['state_dict'])\n",
    "    \n",
    "    checkpoint = torch.load('weights/feature_extractor_params.pth.tar',\n",
    "                             map_location=device)\n",
    "    feature_extractor = feature_extractor().to(device)\n",
    "    feature_extractor.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "elif args.dataset == 'mnist':\n",
    "    from models import Autoencoder, VaDE\n",
    "    VaDE = VaDE().to(device)\n",
    "    autoencoder = Autoencoder().to(device)\n",
    "    autoencoder.load_state_dict(torch.load('weights/autoencoder_parameters_mnist.pth.tar',\n",
    "                                    map_location=device)['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "if args.dataset == 'webcam':\n",
    "    classes = ['back_pack',\n",
    "                'bike',\n",
    "                'bike_helmet',\n",
    "                'bookcase',\n",
    "                'bottle',\n",
    "                'calculator',\n",
    "                'desk_chair',\n",
    "                'desk_lamp',\n",
    "                'desktop_computer',\n",
    "                'file_cabinet',\n",
    "                'headphones',\n",
    "                'keyboard',\n",
    "                'laptop_computer',\n",
    "                'letter_tray',\n",
    "                'mobile_phone',\n",
    "                'monitor',\n",
    "                'mouse',\n",
    "                'mug',\n",
    "                'paper_notebook',\n",
    "                'pen',\n",
    "                'phone',\n",
    "                'printer',\n",
    "                'projector',\n",
    "                'punchers',\n",
    "                'ring_binder',\n",
    "                'ruler',\n",
    "                'scissors',\n",
    "                'speaker',\n",
    "                'stapler',\n",
    "                'tape_dispenser',\n",
    "                'trash_can']\n",
    "else:\n",
    "    classes = ['0',\n",
    "               '1',\n",
    "               '2',\n",
    "               '3',\n",
    "               '4',\n",
    "               '5',\n",
    "               '6',\n",
    "               '7',\n",
    "               '8',\n",
    "               '9']\n",
    "\n",
    "\n",
    "def get_latent_space(dataloader, z_dim, model, device, ftr_ext=None):\n",
    "    z = torch.zeros((1, z_dim)).float().to(device)\n",
    "    y = torch.zeros((1)).long().to(device)\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img, label = img.to(device).float(), label.to(device).long()\n",
    "            if ftr_ext is not None:\n",
    "                img = ftr_ext(img); img = img.detach()\n",
    "\n",
    "            z_l = model.encode(img)\n",
    "            y = torch.cat((y, label), dim=0)\n",
    "            z = torch.cat((z, z_l), dim=0)\n",
    "    return z[1:], y[1:]\n",
    "\n",
    "\n",
    "def plot_tsne(X_embedded, y, ticks):\n",
    "    f, ax1 = plt.subplots(1, 1, sharey=True, figsize=(15,10))\n",
    "\n",
    "    cmap = plt.get_cmap('jet', 31)\n",
    "\n",
    "\n",
    "    cax = ax1.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y.numpy(),\n",
    "                      s=15, cmap=cmap)\n",
    "\n",
    "    cbar = f.colorbar(cax, ticks=np.linspace(0,30,31))\n",
    "    cbar.ax.set_yticklabels(ticks)\n",
    "\n",
    "    ax1.xaxis.set_visible(False)\n",
    "    ax1.yaxis.set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 10\n",
    "\n",
    "z, y = get_latent_space(dataloader_train, z_dim, autoencoder, device)\n",
    "z, y = z.cpu(), y.cpu()\n",
    "#z_embedded = TSNE(n_components=2).fit_transform(z.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_tsne(z_embedded, y, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "var = []\n",
    "proportion = []\n",
    "for i in range(n_classes):\n",
    "    ixs = np.where(y.cpu().numpy() == i)\n",
    "    means.append(torch.mean(z[ixs].detach(), dim=0))\n",
    "    var.append(torch.std(z[ixs].detach(), dim=0)**2)\n",
    "    proportion.append(len(ixs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7225,  1.5110, -1.6059,  0.1572, -1.4409,  2.5070, -0.2138, -1.1498,\n",
       "         -0.4085, -3.0455],\n",
       "        [ 0.5270,  0.3365, -1.3417, -2.1833, -0.0318, -0.1525,  0.3218,  2.1061,\n",
       "         -0.4019,  0.5672],\n",
       "        [-1.2752,  1.0957, -1.7303, -0.7026, -1.9445,  0.0119, -0.8125,  2.1270,\n",
       "          1.2704, -0.2549],\n",
       "        [-0.1403,  0.9838, -0.9453, -0.3247, -0.3575,  2.7426, -1.7609,  1.1172,\n",
       "          0.0941,  0.0744],\n",
       "        [-0.2172, -0.0385, -0.9906, -0.2127, -0.1067,  0.8831,  2.7958,  0.5324,\n",
       "          1.3305, -0.3235],\n",
       "        [ 0.5567, -0.6318, -1.9012, -0.0334, -0.7016,  2.5153, -0.1153,  0.2810,\n",
       "         -0.2717, -1.1620],\n",
       "        [-1.7894, -0.4245, -0.3524, -2.4896, -0.7695,  1.4614,  0.7426,  0.2212,\n",
       "          0.7004, -1.5768],\n",
       "        [-0.3239,  2.3009, -1.5938,  0.5262, -0.7228,  1.1730,  1.8686,  1.1445,\n",
       "         -1.1222,  0.7909],\n",
       "        [-0.3210,  0.4511, -1.7597,  0.0295,  0.9434,  0.9218, -0.1502,  1.6866,\n",
       "          0.5826, -0.9346],\n",
       "        [-0.3713,  0.6447, -0.8550,  0.0778,  0.0731,  1.2775,  2.1856,  0.8330,\n",
       "         -0.3810, -0.2736]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = torch.stack(means)\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4790, 1.3188, 3.9344, 2.2020, 3.5966, 1.1384, 1.1770, 1.3883, 1.2251,\n",
       "         1.8249],\n",
       "        [0.2681, 0.3172, 2.7721, 0.4057, 0.3317, 0.3890, 0.2415, 0.4374, 0.3806,\n",
       "         0.3090],\n",
       "        [2.2401, 1.3323, 2.4983, 1.8782, 2.0342, 1.0204, 1.2757, 1.8976, 1.5967,\n",
       "         1.6970],\n",
       "        [1.0718, 0.8199, 2.5741, 1.0362, 1.1774, 1.5073, 1.1107, 0.8607, 0.8168,\n",
       "         0.8794],\n",
       "        [0.7077, 0.5010, 2.9294, 1.2037, 1.0465, 0.9659, 0.7297, 0.7875, 0.9520,\n",
       "         1.2278],\n",
       "        [1.7126, 1.1105, 4.1666, 1.0752, 1.6680, 1.4329, 0.9751, 0.9790, 0.7626,\n",
       "         0.9162],\n",
       "        [1.6163, 1.4314, 2.0344, 1.4047, 1.4621, 0.9662, 0.5629, 0.9303, 1.3322,\n",
       "         1.2190],\n",
       "        [0.9252, 1.3466, 2.9539, 0.9515, 1.0446, 1.5429, 1.2181, 1.2913, 0.7892,\n",
       "         0.8701],\n",
       "        [0.6719, 0.8895, 2.7315, 0.9546, 0.7133, 0.7882, 0.6740, 0.9194, 0.7710,\n",
       "         0.7292],\n",
       "        [0.6304, 0.3315, 2.5566, 0.6841, 0.7056, 0.8292, 0.8711, 0.4241, 0.3767,\n",
       "         1.1552]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = torch.stack(var)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion = torch.Tensor(proportion)/torch.sum(torch.Tensor(proportion))\n",
    "proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = autoencoder.state_dict()\n",
    "\n",
    "VaDE.load_state_dict(state_dict=state_dict, strict=False)\n",
    "VaDE.pi_prior.data = proportion.float().to(device)\n",
    "VaDE.mu_prior.data = means.float().to(device)\n",
    "VaDE.log_var_prior.data = torch.log(var).float().to(device)\n",
    "torch.save(VaDE.state_dict(), 'weights/pretrained_parameters_{}.pth'.format(args.dataset)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "        0.1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
