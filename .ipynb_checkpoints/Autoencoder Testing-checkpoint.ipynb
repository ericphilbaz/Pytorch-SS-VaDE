{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "from preprocess import get_mnist, get_webcam\n",
    "from train import TrainerVaDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    batch_size = 128\n",
    "    lr = 1e-5\n",
    "    dataset = 'webcam'\n",
    "    pretrained_path = 'weights/pretrained_parameter.pth'\n",
    "    patience = 50\n",
    "    pretrain = True\n",
    "    epochs = 200\n",
    "    n_shots = 1\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    \n",
    "args = Args() # Parsing all the arguments for the training\n",
    "if args.dataset == 'mnist':\n",
    "    dataloader_sup,  dataloader_unsup = get_mnist(args)\n",
    "    n_classes = 10\n",
    "else:\n",
    "    dataloader_sup,  dataloader_unsup = get_webcam(args)\n",
    "    n_classes = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vade = TrainerVaDE(args, device, dataloader_sup, dataloader_unsup, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the autoencoder...\n",
      "Training Autoencoder... Epoch: 0, Loss: 1.803697943687439\n",
      "Training Autoencoder... Epoch: 1, Loss: 1.8257158398628235\n",
      "Training Autoencoder... Epoch: 2, Loss: 1.4806458055973053\n",
      "Training Autoencoder... Epoch: 3, Loss: 1.1924493610858917\n",
      "Training Autoencoder... Epoch: 4, Loss: 1.1023206114768982\n",
      "Training Autoencoder... Epoch: 5, Loss: 1.0719996690750122\n",
      "Training Autoencoder... Epoch: 6, Loss: 1.0587001740932465\n",
      "Training Autoencoder... Epoch: 7, Loss: 1.0493691861629486\n",
      "Training Autoencoder... Epoch: 8, Loss: 1.042820855975151\n",
      "Training Autoencoder... Epoch: 9, Loss: 1.0394747853279114\n",
      "Training Autoencoder... Epoch: 10, Loss: 1.0394821763038635\n",
      "Training Autoencoder... Epoch: 11, Loss: 1.034037783741951\n",
      "Training Autoencoder... Epoch: 12, Loss: 1.0323224365711212\n",
      "Training Autoencoder... Epoch: 13, Loss: 1.0322849303483963\n",
      "Training Autoencoder... Epoch: 14, Loss: 1.0308918505907059\n",
      "Training Autoencoder... Epoch: 15, Loss: 1.0304935425519943\n",
      "Training Autoencoder... Epoch: 16, Loss: 1.0320035815238953\n",
      "Training Autoencoder... Epoch: 17, Loss: 1.0330529808998108\n",
      "Training Autoencoder... Epoch: 18, Loss: 1.0315185189247131\n",
      "Training Autoencoder... Epoch: 19, Loss: 1.0301137417554855\n",
      "Training Autoencoder... Epoch: 20, Loss: 1.0304134637117386\n",
      "Training Autoencoder... Epoch: 21, Loss: 1.0314147770404816\n",
      "Training Autoencoder... Epoch: 22, Loss: 1.0331071764230728\n",
      "Training Autoencoder... Epoch: 23, Loss: 1.0324760377407074\n",
      "Training Autoencoder... Epoch: 24, Loss: 1.0339118093252182\n",
      "Training Autoencoder... Epoch: 25, Loss: 1.0325486958026886\n",
      "Training Autoencoder... Epoch: 26, Loss: 1.0305296778678894\n",
      "Training Autoencoder... Epoch: 27, Loss: 1.028115376830101\n",
      "Training Autoencoder... Epoch: 28, Loss: 1.0274702906608582\n",
      "Training Autoencoder... Epoch: 29, Loss: 1.027163490653038\n",
      "Training Autoencoder... Epoch: 30, Loss: 1.027553454041481\n",
      "Training Autoencoder... Epoch: 31, Loss: 1.027338683605194\n",
      "Training Autoencoder... Epoch: 32, Loss: 1.0273966193199158\n",
      "Training Autoencoder... Epoch: 33, Loss: 1.027866318821907\n",
      "Training Autoencoder... Epoch: 34, Loss: 1.0266136080026627\n",
      "Training Autoencoder... Epoch: 35, Loss: 1.0272891223430634\n",
      "Training Autoencoder... Epoch: 36, Loss: 1.0256097316741943\n",
      "Training Autoencoder... Epoch: 37, Loss: 1.0265847146511078\n",
      "Training Autoencoder... Epoch: 38, Loss: 1.0255217105150223\n",
      "Training Autoencoder... Epoch: 39, Loss: 1.0253059267997742\n",
      "Training Autoencoder... Epoch: 40, Loss: 1.0264742076396942\n",
      "Training Autoencoder... Epoch: 41, Loss: 1.026024505496025\n",
      "Training Autoencoder... Epoch: 42, Loss: 1.0250293761491776\n",
      "Training Autoencoder... Epoch: 43, Loss: 1.0245474725961685\n",
      "Training Autoencoder... Epoch: 44, Loss: 1.0232236981391907\n",
      "Training Autoencoder... Epoch: 45, Loss: 1.0212480127811432\n",
      "Training Autoencoder... Epoch: 46, Loss: 1.0176305621862411\n",
      "Training Autoencoder... Epoch: 47, Loss: 1.0149513483047485\n",
      "Training Autoencoder... Epoch: 48, Loss: 1.0120691359043121\n",
      "Training Autoencoder... Epoch: 49, Loss: 1.0105396509170532\n",
      "Training Autoencoder... Epoch: 50, Loss: 1.0098924785852432\n",
      "Training Autoencoder... Epoch: 51, Loss: 1.006908267736435\n",
      "Training Autoencoder... Epoch: 52, Loss: 1.0052748173475266\n",
      "Training Autoencoder... Epoch: 53, Loss: 1.0032129287719727\n",
      "Training Autoencoder... Epoch: 54, Loss: 1.006505310535431\n",
      "Training Autoencoder... Epoch: 55, Loss: 1.0036723613739014\n",
      "Training Autoencoder... Epoch: 56, Loss: 1.0031744539737701\n",
      "Training Autoencoder... Epoch: 57, Loss: 1.0032749027013779\n",
      "Training Autoencoder... Epoch: 58, Loss: 1.0037686377763748\n",
      "Training Autoencoder... Epoch: 59, Loss: 1.0003912299871445\n",
      "Training Autoencoder... Epoch: 60, Loss: 0.9992453008890152\n",
      "Training Autoencoder... Epoch: 61, Loss: 1.0021482855081558\n",
      "Training Autoencoder... Epoch: 62, Loss: 1.0018927156925201\n",
      "Training Autoencoder... Epoch: 63, Loss: 0.999211847782135\n",
      "Training Autoencoder... Epoch: 64, Loss: 0.9971813261508942\n",
      "Training Autoencoder... Epoch: 65, Loss: 0.9944829940795898\n",
      "Training Autoencoder... Epoch: 66, Loss: 0.9928432255983353\n",
      "Training Autoencoder... Epoch: 67, Loss: 0.9936128556728363\n",
      "Training Autoencoder... Epoch: 68, Loss: 0.9913650006055832\n",
      "Training Autoencoder... Epoch: 69, Loss: 0.9901435226202011\n",
      "Training Autoencoder... Epoch: 70, Loss: 0.9864117503166199\n",
      "Training Autoencoder... Epoch: 71, Loss: 0.9896010160446167\n",
      "Training Autoencoder... Epoch: 72, Loss: 0.9893447011709213\n",
      "Training Autoencoder... Epoch: 73, Loss: 0.9837107211351395\n",
      "Training Autoencoder... Epoch: 74, Loss: 0.9830591529607773\n",
      "Training Autoencoder... Epoch: 75, Loss: 0.9803696274757385\n",
      "Training Autoencoder... Epoch: 76, Loss: 0.9774432927370071\n",
      "Training Autoencoder... Epoch: 77, Loss: 0.9731152653694153\n",
      "Training Autoencoder... Epoch: 78, Loss: 0.9713647812604904\n",
      "Training Autoencoder... Epoch: 79, Loss: 0.9710083454847336\n",
      "Training Autoencoder... Epoch: 80, Loss: 0.9705237001180649\n",
      "Training Autoencoder... Epoch: 81, Loss: 0.9679776430130005\n",
      "Training Autoencoder... Epoch: 82, Loss: 0.9658988416194916\n",
      "Training Autoencoder... Epoch: 83, Loss: 0.9662173837423325\n",
      "Training Autoencoder... Epoch: 84, Loss: 0.9639375805854797\n",
      "Training Autoencoder... Epoch: 85, Loss: 0.9623343348503113\n",
      "Training Autoencoder... Epoch: 86, Loss: 0.9605266898870468\n",
      "Training Autoencoder... Epoch: 87, Loss: 0.9585399031639099\n",
      "Training Autoencoder... Epoch: 88, Loss: 0.9565265476703644\n",
      "Training Autoencoder... Epoch: 89, Loss: 0.9570652991533279\n",
      "Training Autoencoder... Epoch: 90, Loss: 0.9569018185138702\n",
      "Training Autoencoder... Epoch: 91, Loss: 0.9533001631498337\n",
      "Training Autoencoder... Epoch: 92, Loss: 0.9484148472547531\n",
      "Training Autoencoder... Epoch: 93, Loss: 0.943939670920372\n",
      "Training Autoencoder... Epoch: 94, Loss: 0.9428837895393372\n",
      "Training Autoencoder... Epoch: 95, Loss: 0.9417252540588379\n",
      "Training Autoencoder... Epoch: 96, Loss: 0.9401456117630005\n",
      "Training Autoencoder... Epoch: 97, Loss: 0.937774047255516\n",
      "Training Autoencoder... Epoch: 98, Loss: 0.9355450421571732\n",
      "Training Autoencoder... Epoch: 99, Loss: 0.933239683508873\n",
      "Training Autoencoder... Epoch: 100, Loss: 0.9317446351051331\n",
      "Training Autoencoder... Epoch: 101, Loss: 0.9300450533628464\n",
      "Training Autoencoder... Epoch: 102, Loss: 0.9287664890289307\n",
      "Training Autoencoder... Epoch: 103, Loss: 0.9284271150827408\n",
      "Training Autoencoder... Epoch: 104, Loss: 0.928030788898468\n",
      "Training Autoencoder... Epoch: 105, Loss: 0.9294053614139557\n",
      "Training Autoencoder... Epoch: 106, Loss: 0.9256768226623535\n",
      "Training Autoencoder... Epoch: 107, Loss: 0.92298923432827\n",
      "Training Autoencoder... Epoch: 108, Loss: 0.9213990718126297\n",
      "Training Autoencoder... Epoch: 109, Loss: 0.9199119657278061\n",
      "Training Autoencoder... Epoch: 110, Loss: 0.9202617853879929\n",
      "Training Autoencoder... Epoch: 111, Loss: 0.9164846539497375\n",
      "Training Autoencoder... Epoch: 112, Loss: 0.9144216775894165\n",
      "Training Autoencoder... Epoch: 113, Loss: 0.9109329581260681\n",
      "Training Autoencoder... Epoch: 114, Loss: 0.9100287109613419\n",
      "Training Autoencoder... Epoch: 115, Loss: 0.909400075674057\n",
      "Training Autoencoder... Epoch: 116, Loss: 0.9074429571628571\n",
      "Training Autoencoder... Epoch: 117, Loss: 0.9074868857860565\n",
      "Training Autoencoder... Epoch: 118, Loss: 0.9055624157190323\n",
      "Training Autoencoder... Epoch: 119, Loss: 0.9048226177692413\n",
      "Training Autoencoder... Epoch: 120, Loss: 0.9012575745582581\n",
      "Training Autoencoder... Epoch: 121, Loss: 0.900104284286499\n",
      "Training Autoencoder... Epoch: 122, Loss: 0.8969658613204956\n",
      "Training Autoencoder... Epoch: 123, Loss: 0.895174041390419\n",
      "Training Autoencoder... Epoch: 124, Loss: 0.8939983993768692\n",
      "Training Autoencoder... Epoch: 125, Loss: 0.8916832357645035\n",
      "Training Autoencoder... Epoch: 126, Loss: 0.8911652863025665\n",
      "Training Autoencoder... Epoch: 127, Loss: 0.8909237086772919\n",
      "Training Autoencoder... Epoch: 128, Loss: 0.8890960514545441\n",
      "Training Autoencoder... Epoch: 129, Loss: 0.8879328519105911\n",
      "Training Autoencoder... Epoch: 130, Loss: 0.8877314031124115\n",
      "Training Autoencoder... Epoch: 131, Loss: 0.886383593082428\n",
      "Training Autoencoder... Epoch: 132, Loss: 0.8845550417900085\n",
      "Training Autoencoder... Epoch: 133, Loss: 0.8808508515357971\n",
      "Training Autoencoder... Epoch: 134, Loss: 0.8813670426607132\n",
      "Training Autoencoder... Epoch: 135, Loss: 0.8790538609027863\n",
      "Training Autoencoder... Epoch: 136, Loss: 0.8760425299406052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 137, Loss: 0.873703807592392\n",
      "Training Autoencoder... Epoch: 138, Loss: 0.8723009079694748\n",
      "Training Autoencoder... Epoch: 139, Loss: 0.8700720965862274\n",
      "Training Autoencoder... Epoch: 140, Loss: 0.8684542030096054\n",
      "Training Autoencoder... Epoch: 141, Loss: 0.8662021905183792\n",
      "Training Autoencoder... Epoch: 142, Loss: 0.8644860833883286\n",
      "Training Autoencoder... Epoch: 143, Loss: 0.8624795228242874\n",
      "Training Autoencoder... Epoch: 144, Loss: 0.8617046177387238\n",
      "Training Autoencoder... Epoch: 145, Loss: 0.8626348972320557\n",
      "Training Autoencoder... Epoch: 146, Loss: 0.8591481000185013\n",
      "Training Autoencoder... Epoch: 147, Loss: 0.856756791472435\n",
      "Training Autoencoder... Epoch: 148, Loss: 0.8559137433767319\n",
      "Training Autoencoder... Epoch: 149, Loss: 0.8550406396389008\n",
      "Training Autoencoder... Epoch: 150, Loss: 0.8523805737495422\n",
      "Training Autoencoder... Epoch: 151, Loss: 0.8514618426561356\n",
      "Training Autoencoder... Epoch: 152, Loss: 0.8497017174959183\n",
      "Training Autoencoder... Epoch: 153, Loss: 0.8468653112649918\n",
      "Training Autoencoder... Epoch: 154, Loss: 0.8463791906833649\n",
      "Training Autoencoder... Epoch: 155, Loss: 0.8453757911920547\n",
      "Training Autoencoder... Epoch: 156, Loss: 0.8449131846427917\n",
      "Training Autoencoder... Epoch: 157, Loss: 0.8441404402256012\n",
      "Training Autoencoder... Epoch: 158, Loss: 0.8422931879758835\n",
      "Training Autoencoder... Epoch: 159, Loss: 0.8398875445127487\n",
      "Training Autoencoder... Epoch: 160, Loss: 0.8373582065105438\n",
      "Training Autoencoder... Epoch: 161, Loss: 0.8358840346336365\n",
      "Training Autoencoder... Epoch: 162, Loss: 0.8329624384641647\n",
      "Training Autoencoder... Epoch: 163, Loss: 0.8320064544677734\n",
      "Training Autoencoder... Epoch: 164, Loss: 0.833033099770546\n",
      "Training Autoencoder... Epoch: 165, Loss: 0.8320242166519165\n",
      "Training Autoencoder... Epoch: 166, Loss: 0.8303024768829346\n",
      "Training Autoencoder... Epoch: 167, Loss: 0.8278483599424362\n",
      "Training Autoencoder... Epoch: 168, Loss: 0.8261387944221497\n",
      "Training Autoencoder... Epoch: 169, Loss: 0.8253254890441895\n",
      "Training Autoencoder... Epoch: 170, Loss: 0.8226809501647949\n",
      "Training Autoencoder... Epoch: 171, Loss: 0.822380855679512\n",
      "Training Autoencoder... Epoch: 172, Loss: 0.8209985941648483\n",
      "Training Autoencoder... Epoch: 173, Loss: 0.8206343948841095\n",
      "Training Autoencoder... Epoch: 174, Loss: 0.8203930407762527\n",
      "Training Autoencoder... Epoch: 175, Loss: 0.8199904710054398\n",
      "Training Autoencoder... Epoch: 176, Loss: 0.8189647197723389\n",
      "Training Autoencoder... Epoch: 177, Loss: 0.8164775222539902\n",
      "Training Autoencoder... Epoch: 178, Loss: 0.8154182881116867\n",
      "Training Autoencoder... Epoch: 179, Loss: 0.8136802464723587\n",
      "Training Autoencoder... Epoch: 180, Loss: 0.8131511062383652\n",
      "Training Autoencoder... Epoch: 181, Loss: 0.8125986009836197\n",
      "Training Autoencoder... Epoch: 182, Loss: 0.811171367764473\n",
      "Training Autoencoder... Epoch: 183, Loss: 0.8101902008056641\n",
      "Training Autoencoder... Epoch: 184, Loss: 0.8095644116401672\n",
      "Training Autoencoder... Epoch: 185, Loss: 0.8080864101648331\n",
      "Training Autoencoder... Epoch: 186, Loss: 0.8079985678195953\n",
      "Training Autoencoder... Epoch: 187, Loss: 0.8064367324113846\n",
      "Training Autoencoder... Epoch: 188, Loss: 0.8056470155715942\n",
      "Training Autoencoder... Epoch: 189, Loss: 0.8036860376596451\n",
      "Training Autoencoder... Epoch: 190, Loss: 0.8023993372917175\n",
      "Training Autoencoder... Epoch: 191, Loss: 0.8024624288082123\n",
      "Training Autoencoder... Epoch: 192, Loss: 0.7999022752046585\n",
      "Training Autoencoder... Epoch: 193, Loss: 0.7985672652721405\n",
      "Training Autoencoder... Epoch: 194, Loss: 0.7976526021957397\n",
      "Training Autoencoder... Epoch: 195, Loss: 0.7981301099061966\n",
      "Training Autoencoder... Epoch: 196, Loss: 0.7969382703304291\n",
      "Training Autoencoder... Epoch: 197, Loss: 0.796248123049736\n",
      "Training Autoencoder... Epoch: 198, Loss: 0.7947587072849274\n",
      "Training Autoencoder... Epoch: 199, Loss: 0.7940188944339752\n",
      "Training Autoencoder... Epoch: 200, Loss: 0.7944090515375137\n",
      "Training Autoencoder... Epoch: 201, Loss: 0.7942838817834854\n",
      "Training Autoencoder... Epoch: 202, Loss: 0.794520303606987\n",
      "Training Autoencoder... Epoch: 203, Loss: 0.7924657911062241\n",
      "Training Autoencoder... Epoch: 204, Loss: 0.7923988550901413\n",
      "Training Autoencoder... Epoch: 205, Loss: 0.7918802499771118\n",
      "Training Autoencoder... Epoch: 206, Loss: 0.7897409349679947\n",
      "Training Autoencoder... Epoch: 207, Loss: 0.7885619252920151\n",
      "Training Autoencoder... Epoch: 208, Loss: 0.7873295992612839\n",
      "Training Autoencoder... Epoch: 209, Loss: 0.7857001423835754\n",
      "Training Autoencoder... Epoch: 210, Loss: 0.7856310158967972\n",
      "Training Autoencoder... Epoch: 211, Loss: 0.7850389778614044\n",
      "Training Autoencoder... Epoch: 212, Loss: 0.7847308367490768\n",
      "Training Autoencoder... Epoch: 213, Loss: 0.7839151322841644\n",
      "Training Autoencoder... Epoch: 214, Loss: 0.7833802700042725\n",
      "Training Autoencoder... Epoch: 215, Loss: 0.7818569540977478\n",
      "Training Autoencoder... Epoch: 216, Loss: 0.7822149842977524\n",
      "Training Autoencoder... Epoch: 217, Loss: 0.7811335623264313\n",
      "Training Autoencoder... Epoch: 218, Loss: 0.780570462346077\n",
      "Training Autoencoder... Epoch: 219, Loss: 0.7811010926961899\n",
      "Training Autoencoder... Epoch: 220, Loss: 0.7797137200832367\n",
      "Training Autoencoder... Epoch: 221, Loss: 0.7798625230789185\n",
      "Training Autoencoder... Epoch: 222, Loss: 0.7777332216501236\n",
      "Training Autoencoder... Epoch: 223, Loss: 0.7764782905578613\n",
      "Training Autoencoder... Epoch: 224, Loss: 0.7760642915964127\n",
      "Training Autoencoder... Epoch: 225, Loss: 0.7746110558509827\n",
      "Training Autoencoder... Epoch: 226, Loss: 0.7733698934316635\n",
      "Training Autoencoder... Epoch: 227, Loss: 0.7728482037782669\n",
      "Training Autoencoder... Epoch: 228, Loss: 0.7720739096403122\n",
      "Training Autoencoder... Epoch: 229, Loss: 0.7725999355316162\n",
      "Training Autoencoder... Epoch: 230, Loss: 0.7733932584524155\n",
      "Training Autoencoder... Epoch: 231, Loss: 0.7734804451465607\n",
      "Training Autoencoder... Epoch: 232, Loss: 0.7720227986574173\n",
      "Training Autoencoder... Epoch: 233, Loss: 0.771297812461853\n",
      "Training Autoencoder... Epoch: 234, Loss: 0.7709652632474899\n",
      "Training Autoencoder... Epoch: 235, Loss: 0.7698177248239517\n",
      "Training Autoencoder... Epoch: 236, Loss: 0.7696777433156967\n",
      "Training Autoencoder... Epoch: 237, Loss: 0.7676594778895378\n",
      "Training Autoencoder... Epoch: 238, Loss: 0.7663995921611786\n",
      "Training Autoencoder... Epoch: 239, Loss: 0.7666200995445251\n",
      "Training Autoencoder... Epoch: 240, Loss: 0.7687477022409439\n",
      "Training Autoencoder... Epoch: 241, Loss: 0.7683060616254807\n",
      "Training Autoencoder... Epoch: 242, Loss: 0.768082931637764\n",
      "Training Autoencoder... Epoch: 243, Loss: 0.7667593508958817\n",
      "Training Autoencoder... Epoch: 244, Loss: 0.7655949145555496\n",
      "Training Autoencoder... Epoch: 245, Loss: 0.7637714669108391\n",
      "Training Autoencoder... Epoch: 246, Loss: 0.762693464756012\n",
      "Training Autoencoder... Epoch: 247, Loss: 0.7613785117864609\n",
      "Training Autoencoder... Epoch: 248, Loss: 0.7615025490522385\n",
      "Training Autoencoder... Epoch: 249, Loss: 0.7612374126911163\n",
      "Training Autoencoder... Epoch: 250, Loss: 0.76039669662714\n",
      "Training Autoencoder... Epoch: 251, Loss: 0.7607095539569855\n",
      "Training Autoencoder... Epoch: 252, Loss: 0.7596675157546997\n",
      "Training Autoencoder... Epoch: 253, Loss: 0.7591294646263123\n",
      "Training Autoencoder... Epoch: 254, Loss: 0.7580954283475876\n",
      "Training Autoencoder... Epoch: 255, Loss: 0.7571955174207687\n",
      "Training Autoencoder... Epoch: 256, Loss: 0.7556428462266922\n",
      "Training Autoencoder... Epoch: 257, Loss: 0.7556160986423492\n",
      "Training Autoencoder... Epoch: 258, Loss: 0.7548184469342232\n",
      "Training Autoencoder... Epoch: 259, Loss: 0.7554291188716888\n",
      "Training Autoencoder... Epoch: 260, Loss: 0.7574204578995705\n",
      "Training Autoencoder... Epoch: 261, Loss: 0.7565623968839645\n",
      "Training Autoencoder... Epoch: 262, Loss: 0.7566367015242577\n",
      "Training Autoencoder... Epoch: 263, Loss: 0.7552173212170601\n",
      "Training Autoencoder... Epoch: 264, Loss: 0.7541909143328667\n",
      "Training Autoencoder... Epoch: 265, Loss: 0.7533630579710007\n",
      "Training Autoencoder... Epoch: 266, Loss: 0.7513483762741089\n",
      "Training Autoencoder... Epoch: 267, Loss: 0.7520207762718201\n",
      "Training Autoencoder... Epoch: 268, Loss: 0.7525824159383774\n",
      "Training Autoencoder... Epoch: 269, Loss: 0.7527792751789093\n",
      "Training Autoencoder... Epoch: 270, Loss: 0.752233199775219\n",
      "Training Autoencoder... Epoch: 271, Loss: 0.7508993223309517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 272, Loss: 0.7510630935430527\n",
      "Training Autoencoder... Epoch: 273, Loss: 0.7497488707304001\n",
      "Training Autoencoder... Epoch: 274, Loss: 0.7500879839062691\n",
      "Training Autoencoder... Epoch: 275, Loss: 0.7499596178531647\n",
      "Training Autoencoder... Epoch: 276, Loss: 0.7491324841976166\n",
      "Training Autoencoder... Epoch: 277, Loss: 0.7490551471710205\n",
      "Training Autoencoder... Epoch: 278, Loss: 0.7483705431222916\n",
      "Training Autoencoder... Epoch: 279, Loss: 0.7469807714223862\n",
      "Training Autoencoder... Epoch: 280, Loss: 0.7470328211784363\n",
      "Training Autoencoder... Epoch: 281, Loss: 0.7471618056297302\n",
      "Training Autoencoder... Epoch: 282, Loss: 0.7472570911049843\n",
      "Training Autoencoder... Epoch: 283, Loss: 0.745681457221508\n",
      "Training Autoencoder... Epoch: 284, Loss: 0.7451033815741539\n",
      "Training Autoencoder... Epoch: 285, Loss: 0.746335431933403\n",
      "Training Autoencoder... Epoch: 286, Loss: 0.7439364567399025\n",
      "Training Autoencoder... Epoch: 287, Loss: 0.7450397908687592\n",
      "Training Autoencoder... Epoch: 288, Loss: 0.7438788339495659\n",
      "Training Autoencoder... Epoch: 289, Loss: 0.7433596253395081\n",
      "Training Autoencoder... Epoch: 290, Loss: 0.7428041324019432\n",
      "Training Autoencoder... Epoch: 291, Loss: 0.7429937198758125\n",
      "Training Autoencoder... Epoch: 292, Loss: 0.7420912533998489\n",
      "Training Autoencoder... Epoch: 293, Loss: 0.7412195429205894\n",
      "Training Autoencoder... Epoch: 294, Loss: 0.7406317442655563\n",
      "Training Autoencoder... Epoch: 295, Loss: 0.7412756755948067\n",
      "Training Autoencoder... Epoch: 296, Loss: 0.74199628084898\n",
      "Training Autoencoder... Epoch: 297, Loss: 0.7418711259961128\n",
      "Training Autoencoder... Epoch: 298, Loss: 0.7421742007136345\n",
      "Training Autoencoder... Epoch: 299, Loss: 0.7427794188261032\n",
      "Training Autoencoder... Epoch: 300, Loss: 0.7416534125804901\n",
      "Training Autoencoder... Epoch: 301, Loss: 0.7405933812260628\n",
      "Training Autoencoder... Epoch: 302, Loss: 0.739809475839138\n",
      "Training Autoencoder... Epoch: 303, Loss: 0.7385690733790398\n",
      "Training Autoencoder... Epoch: 304, Loss: 0.7378397360444069\n",
      "Training Autoencoder... Epoch: 305, Loss: 0.737422913312912\n",
      "Training Autoencoder... Epoch: 306, Loss: 0.7384268641471863\n",
      "Training Autoencoder... Epoch: 307, Loss: 0.7383103370666504\n",
      "Training Autoencoder... Epoch: 308, Loss: 0.7364429906010628\n",
      "Training Autoencoder... Epoch: 309, Loss: 0.7366292774677277\n",
      "Training Autoencoder... Epoch: 310, Loss: 0.7362378463149071\n",
      "Training Autoencoder... Epoch: 311, Loss: 0.7363859415054321\n",
      "Training Autoencoder... Epoch: 312, Loss: 0.735694907605648\n",
      "Training Autoencoder... Epoch: 313, Loss: 0.7346508353948593\n",
      "Training Autoencoder... Epoch: 314, Loss: 0.7356559634208679\n",
      "Training Autoencoder... Epoch: 315, Loss: 0.7359314188361168\n",
      "Training Autoencoder... Epoch: 316, Loss: 0.7352999374270439\n",
      "Training Autoencoder... Epoch: 317, Loss: 0.7347161993384361\n",
      "Training Autoencoder... Epoch: 318, Loss: 0.7350769639015198\n",
      "Training Autoencoder... Epoch: 319, Loss: 0.7331856936216354\n",
      "Training Autoencoder... Epoch: 320, Loss: 0.7333781570196152\n",
      "Training Autoencoder... Epoch: 321, Loss: 0.7343970313668251\n",
      "Training Autoencoder... Epoch: 322, Loss: 0.7339978739619255\n",
      "Training Autoencoder... Epoch: 323, Loss: 0.7329790741205215\n",
      "Training Autoencoder... Epoch: 324, Loss: 0.7336024791002274\n",
      "Training Autoencoder... Epoch: 325, Loss: 0.7329412326216698\n",
      "Training Autoencoder... Epoch: 326, Loss: 0.7321711853146553\n",
      "Training Autoencoder... Epoch: 327, Loss: 0.7314816266298294\n",
      "Training Autoencoder... Epoch: 328, Loss: 0.7313809096813202\n",
      "Training Autoencoder... Epoch: 329, Loss: 0.7306949868798256\n",
      "Training Autoencoder... Epoch: 330, Loss: 0.7306485772132874\n",
      "Training Autoencoder... Epoch: 331, Loss: 0.7299709841609001\n",
      "Training Autoencoder... Epoch: 332, Loss: 0.7291609421372414\n",
      "Training Autoencoder... Epoch: 333, Loss: 0.7302069738507271\n",
      "Training Autoencoder... Epoch: 334, Loss: 0.7301743105053902\n",
      "Training Autoencoder... Epoch: 335, Loss: 0.7301490530371666\n",
      "Training Autoencoder... Epoch: 336, Loss: 0.7283338010311127\n",
      "Training Autoencoder... Epoch: 337, Loss: 0.7280915826559067\n",
      "Training Autoencoder... Epoch: 338, Loss: 0.7284745201468468\n",
      "Training Autoencoder... Epoch: 339, Loss: 0.7282344475388527\n",
      "Training Autoencoder... Epoch: 340, Loss: 0.7277599945664406\n",
      "Training Autoencoder... Epoch: 341, Loss: 0.7272379323840141\n",
      "Training Autoencoder... Epoch: 342, Loss: 0.7264191806316376\n",
      "Training Autoencoder... Epoch: 343, Loss: 0.7272666990756989\n",
      "Training Autoencoder... Epoch: 344, Loss: 0.7255565896630287\n",
      "Training Autoencoder... Epoch: 345, Loss: 0.7259606197476387\n",
      "Training Autoencoder... Epoch: 346, Loss: 0.7256780564785004\n",
      "Training Autoencoder... Epoch: 347, Loss: 0.7273920774459839\n",
      "Training Autoencoder... Epoch: 348, Loss: 0.726371057331562\n",
      "Training Autoencoder... Epoch: 349, Loss: 0.7260437980294228\n",
      "Training Autoencoder... Epoch: 350, Loss: 0.7263749092817307\n",
      "Training Autoencoder... Epoch: 351, Loss: 0.7271756529808044\n",
      "Training Autoencoder... Epoch: 352, Loss: 0.725697249174118\n",
      "Training Autoencoder... Epoch: 353, Loss: 0.7237571030855179\n",
      "Training Autoencoder... Epoch: 354, Loss: 0.7254328578710556\n",
      "Training Autoencoder... Epoch: 355, Loss: 0.7240055650472641\n",
      "Training Autoencoder... Epoch: 356, Loss: 0.7241608425974846\n",
      "Training Autoencoder... Epoch: 357, Loss: 0.7235408797860146\n",
      "Training Autoencoder... Epoch: 358, Loss: 0.7244352251291275\n",
      "Training Autoencoder... Epoch: 359, Loss: 0.7231986597180367\n",
      "Training Autoencoder... Epoch: 360, Loss: 0.7238304540514946\n",
      "Training Autoencoder... Epoch: 361, Loss: 0.7242268696427345\n",
      "Training Autoencoder... Epoch: 362, Loss: 0.722631923854351\n",
      "Training Autoencoder... Epoch: 363, Loss: 0.7229427471756935\n",
      "Training Autoencoder... Epoch: 364, Loss: 0.7218676805496216\n",
      "Training Autoencoder... Epoch: 365, Loss: 0.7225207015872002\n",
      "Training Autoencoder... Epoch: 366, Loss: 0.7222181186079979\n",
      "Training Autoencoder... Epoch: 367, Loss: 0.7210356742143631\n",
      "Training Autoencoder... Epoch: 368, Loss: 0.719800554215908\n",
      "Training Autoencoder... Epoch: 369, Loss: 0.7196572348475456\n",
      "Training Autoencoder... Epoch: 370, Loss: 0.7189297750592232\n",
      "Training Autoencoder... Epoch: 371, Loss: 0.7183276861906052\n",
      "Training Autoencoder... Epoch: 372, Loss: 0.7197404056787491\n",
      "Training Autoencoder... Epoch: 373, Loss: 0.7194329723715782\n",
      "Training Autoencoder... Epoch: 374, Loss: 0.7185971513390541\n",
      "Training Autoencoder... Epoch: 375, Loss: 0.7187865674495697\n",
      "Training Autoencoder... Epoch: 376, Loss: 0.71802569180727\n",
      "Training Autoencoder... Epoch: 377, Loss: 0.7179186418652534\n",
      "Training Autoencoder... Epoch: 378, Loss: 0.7181295156478882\n",
      "Training Autoencoder... Epoch: 379, Loss: 0.7186999842524529\n",
      "Training Autoencoder... Epoch: 380, Loss: 0.7194982394576073\n",
      "Training Autoencoder... Epoch: 381, Loss: 0.7189195677638054\n",
      "Training Autoencoder... Epoch: 382, Loss: 0.7180368527770042\n",
      "Training Autoencoder... Epoch: 383, Loss: 0.7180201709270477\n",
      "Training Autoencoder... Epoch: 384, Loss: 0.7167332097887993\n",
      "Training Autoencoder... Epoch: 385, Loss: 0.7150116935372353\n",
      "Training Autoencoder... Epoch: 386, Loss: 0.7163097336888313\n",
      "Training Autoencoder... Epoch: 387, Loss: 0.7155717685818672\n",
      "Training Autoencoder... Epoch: 388, Loss: 0.7156757563352585\n",
      "Training Autoencoder... Epoch: 389, Loss: 0.7151143848896027\n",
      "Training Autoencoder... Epoch: 390, Loss: 0.7152240127325058\n",
      "Training Autoencoder... Epoch: 391, Loss: 0.7155470997095108\n",
      "Training Autoencoder... Epoch: 392, Loss: 0.715936079621315\n",
      "Training Autoencoder... Epoch: 393, Loss: 0.7147903442382812\n",
      "Training Autoencoder... Epoch: 394, Loss: 0.7139350697398186\n",
      "Training Autoencoder... Epoch: 395, Loss: 0.7152468413114548\n",
      "Training Autoencoder... Epoch: 396, Loss: 0.715668112039566\n",
      "Training Autoencoder... Epoch: 397, Loss: 0.7144802510738373\n",
      "Training Autoencoder... Epoch: 398, Loss: 0.7137261480093002\n",
      "Training Autoencoder... Epoch: 399, Loss: 0.7122815474867821\n",
      "Training Autoencoder... Epoch: 400, Loss: 0.712809182703495\n",
      "Training Autoencoder... Epoch: 401, Loss: 0.7127440348267555\n",
      "Training Autoencoder... Epoch: 402, Loss: 0.712796151638031\n",
      "Training Autoencoder... Epoch: 403, Loss: 0.7136225700378418\n",
      "Training Autoencoder... Epoch: 404, Loss: 0.713621512055397\n",
      "Training Autoencoder... Epoch: 405, Loss: 0.7127211689949036\n",
      "Training Autoencoder... Epoch: 406, Loss: 0.7121031284332275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Autoencoder... Epoch: 407, Loss: 0.7122899442911148\n",
      "Training Autoencoder... Epoch: 408, Loss: 0.7122666388750076\n",
      "Training Autoencoder... Epoch: 409, Loss: 0.7124874740839005\n",
      "Training Autoencoder... Epoch: 410, Loss: 0.7123860195279121\n",
      "Training Autoencoder... Epoch: 411, Loss: 0.7112501785159111\n",
      "Training Autoencoder... Epoch: 412, Loss: 0.7106189653277397\n",
      "Training Autoencoder... Epoch: 413, Loss: 0.7101248577237129\n",
      "Training Autoencoder... Epoch: 414, Loss: 0.7108095437288284\n",
      "Training Autoencoder... Epoch: 415, Loss: 0.7091910913586617\n",
      "Training Autoencoder... Epoch: 416, Loss: 0.7096416652202606\n",
      "Training Autoencoder... Epoch: 417, Loss: 0.7086652517318726\n",
      "Training Autoencoder... Epoch: 418, Loss: 0.7082193791866302\n",
      "Training Autoencoder... Epoch: 419, Loss: 0.7089753523468971\n",
      "Training Autoencoder... Epoch: 420, Loss: 0.7092218995094299\n",
      "Training Autoencoder... Epoch: 421, Loss: 0.7091434970498085\n",
      "Training Autoencoder... Epoch: 422, Loss: 0.7084953486919403\n",
      "Training Autoencoder... Epoch: 423, Loss: 0.7080767825245857\n",
      "Training Autoencoder... Epoch: 424, Loss: 0.7095937877893448\n",
      "Training Autoencoder... Epoch: 425, Loss: 0.7085683345794678\n",
      "Training Autoencoder... Epoch: 426, Loss: 0.7084815502166748\n",
      "Training Autoencoder... Epoch: 427, Loss: 0.7092631831765175\n",
      "Training Autoencoder... Epoch: 428, Loss: 0.7083138972520828\n",
      "Training Autoencoder... Epoch: 429, Loss: 0.7077120169997215\n",
      "Training Autoencoder... Epoch: 430, Loss: 0.7074280604720116\n",
      "Training Autoencoder... Epoch: 431, Loss: 0.7083795070648193\n",
      "Training Autoencoder... Epoch: 432, Loss: 0.7077050730586052\n",
      "Training Autoencoder... Epoch: 433, Loss: 0.7101674005389214\n",
      "Training Autoencoder... Epoch: 434, Loss: 0.7087188959121704\n",
      "Training Autoencoder... Epoch: 435, Loss: 0.7088377252221107\n",
      "Training Autoencoder... Epoch: 436, Loss: 0.7078774198889732\n",
      "Training Autoencoder... Epoch: 437, Loss: 0.7071433961391449\n",
      "Training Autoencoder... Epoch: 438, Loss: 0.7063302174210548\n",
      "Training Autoencoder... Epoch: 439, Loss: 0.7068806663155556\n",
      "Training Autoencoder... Epoch: 440, Loss: 0.7050958424806595\n",
      "Training Autoencoder... Epoch: 441, Loss: 0.705151304602623\n",
      "Training Autoencoder... Epoch: 442, Loss: 0.7046495899558067\n",
      "Training Autoencoder... Epoch: 443, Loss: 0.7052513435482979\n",
      "Training Autoencoder... Epoch: 444, Loss: 0.7040510922670364\n",
      "Training Autoencoder... Epoch: 445, Loss: 0.704705536365509\n",
      "Training Autoencoder... Epoch: 446, Loss: 0.7052680104970932\n",
      "Training Autoencoder... Epoch: 447, Loss: 0.7046812549233437\n",
      "Training Autoencoder... Epoch: 448, Loss: 0.7040781900286674\n"
     ]
    }
   ],
   "source": [
    "vade.pretrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "\n",
    "if args.dataset == 'webcam':\n",
    "    classes = ['back_pack',\n",
    "                'bike',\n",
    "                'bike_helmet',\n",
    "                'bookcase',\n",
    "                'bottle',\n",
    "                'calculator',\n",
    "                'desk_chair',\n",
    "                'desk_lamp',\n",
    "                'desktop_computer',\n",
    "                'file_cabinet',\n",
    "                'headphones',\n",
    "                'keyboard',\n",
    "                'laptop_computer',\n",
    "                'letter_tray',\n",
    "                'mobile_phone',\n",
    "                'monitor',\n",
    "                'mouse',\n",
    "                'mug',\n",
    "                'paper_notebook',\n",
    "                'pen',\n",
    "                'phone',\n",
    "                'printer',\n",
    "                'projector',\n",
    "                'punchers',\n",
    "                'ring_binder',\n",
    "                'ruler',\n",
    "                'scissors',\n",
    "                'speaker',\n",
    "                'stapler',\n",
    "                'tape_dispenser',\n",
    "                'trash_can']\n",
    "else:\n",
    "    classes = ['0',\n",
    "               '1',\n",
    "               '2',\n",
    "               '3',\n",
    "               '4',\n",
    "               '5',\n",
    "               '6',\n",
    "               '7',\n",
    "               '8',\n",
    "               '9']\n",
    "\n",
    "\n",
    "def get_latent_space(dataloader, z_dim, model, device, ftr_ext=None):\n",
    "    z = torch.zeros((1, z_dim)).float().to(device)\n",
    "    y = torch.zeros((1)).long().to(device)\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img, label = img.to(device).float(), label.to(device).long()\n",
    "            if ftr_ext is not None:\n",
    "                img = ftr_ext(img); img = img.detach()\n",
    "\n",
    "            z_l = model.encode(img)\n",
    "            y = torch.cat((y, label), dim=0)\n",
    "            z = torch.cat((z, z_l), dim=0)\n",
    "    return z[1:], y[1:]\n",
    "\n",
    "\n",
    "def plot_tsne(X_embedded, y, ticks):\n",
    "    f, ax1 = plt.subplots(1, 1, sharey=True, figsize=(15,5))\n",
    "\n",
    "    cmap = plt.get_cmap('jet', 31)\n",
    "\n",
    "\n",
    "    cax = ax1.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y.numpy(),\n",
    "                      s=15, cmap=cmap)\n",
    "\n",
    "    cbar = f.colorbar(cax, ticks=np.linspace(0,30,31))\n",
    "    cbar.ax.set_yticklabels(ticks)\n",
    "\n",
    "    ax1.xaxis.set_visible(False)\n",
    "    ax1.yaxis.set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 31\n",
    "model = vade.autoencoder\n",
    "ftr_ext = vade.feature_extractor\n",
    "z, y = get_latent_space(dataloader, z_dim, model, device, ftr_ext)\n",
    "z, y = z.cpu(), y.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_embedded = TSNE(n_components=2).fit_transform(z.detach().numpy())\n",
    "plot_tsne(z_embedded, y, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
